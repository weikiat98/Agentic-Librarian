{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e2e5dc",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-size:3em; font-weight:bold;\">\n",
    "Alpaca GPT LLM Fine Tuning\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171916f",
   "metadata": {},
   "source": [
    "The script is a large language model (LLM) experiment designed to fine-tune LLMs —specifically, Google’s gemma-7b—using the Alpaca GPT-4 dataset. It leverages parameter-efficient fine-tuning (PEFT) with LoRA (Low-Rank Adaptation) and 8-bit quantization for efficient training on consumer GPUs. The script also features implementing checkpointing to allow training to resume after interruptions as fine-tuning can be time consuming.\n",
    "\n",
    "It is possible to modify the script to test different LLMs. Below are some proposed ideas:\n",
    "1. Experiment with Different Base Models for performance and resource requirements comparison\n",
    "2. Hyperparameter Tuning of LLM characteristics\n",
    "3. Evaluation and Validation of model performance during and after training\n",
    "4. Custom Dataset Integration\n",
    "5. Further Advanced Training Techniques\n",
    "6. Robust Checkpoint Management enhancement\n",
    "\n",
    "### changes implemented so far:\n",
    "    # changed epoch from 3 to 1 to shorten timing to train to test stability\n",
    "    # implemented checkpoints for training to allow batch by batch training instead of single training\n",
    "    # allow model to resume training from checkpoints\n",
    "    # saved the last 3 checkpoints temporarily as backup in case crash or pause training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for alpaca gpt4\n",
    "\n",
    "## install required libraries first\n",
    "# pip install transformers datasets accelerate peft trl bitsandbytes numpy\n",
    "# pip install -U numpy==1.23.5 (might skip this command since it was forked from AWS)\n",
    "\n",
    "## run this command to check gpu usage in terminal\n",
    "# nvidia-smi\n",
    "\n",
    "## hugging face access token: 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\n",
    "\n",
    "### Please note that this script is designed to run on a machine with a GPU. Hide hugging face token before sharing the code publicly.\n",
    "\n",
    "# import required libraries and environment setup required for LLM fine tuning\n",
    "import torch\n",
    "import peft\n",
    "import bitsandbytes as bnb\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import whoami\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "hface_access_token = \"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\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if details and setup is correct\n",
    "login(hface_access_token)\n",
    "print(whoami())\n",
    "print(torch.cuda.is_available())\n",
    "print(peft.__version__)\n",
    "print(bnb.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ac4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset alpaca gpt4 from hugging face\n",
    "ds = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
    "\n",
    "def format_example(example):\n",
    "    prompt = example[\"instruction\"]\n",
    "    if example[\"input\"]:\n",
    "        prompt += f\"\\n\\nInput:\\n{example['input']}\"\n",
    "    prompt += \"\\n\\nResponse:\"\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "dataset = ds[\"train\"].map(format_example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 6 rows of the training split\n",
    "for i in range(6):\n",
    "    print(f\"--- Row {i + 1} ---\")\n",
    "    print(\"Instruction:\", ds[\"train\"][i][\"instruction\"])\n",
    "    print(\"Input      :\", ds[\"train\"][i][\"input\"])\n",
    "    print(\"Output     :\", ds[\"train\"][i][\"output\"])\n",
    "    print()\n",
    "\n",
    "model_name = \"google/gemma-7b\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 8-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fcc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with quantization config\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PEFT LoRA support for fine-tuning 8-bit models\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cea3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Dataset for Training\n",
    "def tokenize(batch):\n",
    "    full_texts = [prompt + response for prompt, response in zip(batch[\"prompt\"], batch[\"response\"])]\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma7b-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,  # changed from 3 to 1 for shorter initial training\n",
    "    logging_steps=10,   # log documentation process in intervals of 10\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    resume_from_checkpoint=True  # enable training continuation from last checkpoint\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162aef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training with checkpoint resume support, do not run if training was not done\n",
    "#trainer.train(resume_from_checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
